{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"7eb102530b70401b86763598ab4f301b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3cb2c9f0e914f0e87926914de405303","IPY_MODEL_dd564ebfde7c4968acab7f1c3f7c0566","IPY_MODEL_be6f187690bf4008b41b86806d7e4a1d"],"layout":"IPY_MODEL_b1d9a500880a4aa1904b36f11fab8941"}},"b3cb2c9f0e914f0e87926914de405303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42d0690b09e54f42a25b21a5a5f02838","placeholder":"​","style":"IPY_MODEL_dfc534f8a8a740c69925cde6e5ba9b9b","value":"Downloading: 100%"}},"dd564ebfde7c4968acab7f1c3f7c0566":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3adde2b77054bc584a5ee0b8ad9a8ce","max":630,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39101fbb9e9f48dda4c4525274c37bce","value":630}},"be6f187690bf4008b41b86806d7e4a1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb2669bed87b47f39ae14d5b22d4eed3","placeholder":"​","style":"IPY_MODEL_62b90f280395404f80ca07e351baaef8","value":" 630/630 [00:00&lt;00:00, 21.7kB/s]"}},"b1d9a500880a4aa1904b36f11fab8941":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42d0690b09e54f42a25b21a5a5f02838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc534f8a8a740c69925cde6e5ba9b9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3adde2b77054bc584a5ee0b8ad9a8ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39101fbb9e9f48dda4c4525274c37bce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb2669bed87b47f39ae14d5b22d4eed3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62b90f280395404f80ca07e351baaef8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c33ebaf403084905a8312dbcc63f5003":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b540b483b1d244e7847d05338335e5ac","IPY_MODEL_4081629bc2d34192863bc5b1f735ae29","IPY_MODEL_72c35411cf2941dbbdcaca1f8f30b849"],"layout":"IPY_MODEL_33b88e5ccd3d47dca6ded7e418095884"}},"b540b483b1d244e7847d05338335e5ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfdeba944b5542dd806779cdf00db68c","placeholder":"​","style":"IPY_MODEL_b656607da5b74226b3fd7b7abf921d35","value":"Downloading: 100%"}},"4081629bc2d34192863bc5b1f735ae29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d7e4f9b29a24088ba1d976a5bd321ac","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29f0d28e130e4a4883a6461756ffe0c2","value":231508}},"72c35411cf2941dbbdcaca1f8f30b849":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d63101b99c40ce8d5d0f97a33703c7","placeholder":"​","style":"IPY_MODEL_ff5ae31ae002433a982e4cc4cf389244","value":" 232k/232k [00:00&lt;00:00, 648kB/s]"}},"33b88e5ccd3d47dca6ded7e418095884":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfdeba944b5542dd806779cdf00db68c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b656607da5b74226b3fd7b7abf921d35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d7e4f9b29a24088ba1d976a5bd321ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29f0d28e130e4a4883a6461756ffe0c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86d63101b99c40ce8d5d0f97a33703c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff5ae31ae002433a982e4cc4cf389244":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"480cfc1835cd41cf994ee07a0258cd25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee23da2587174c5b9a051c4a288b0cf1","IPY_MODEL_489a5866c4ee4174aa375d58c278d117","IPY_MODEL_7a4a46aaba124d9e9161f1ee38b4e465"],"layout":"IPY_MODEL_dd596aa7aa004ce88385cef1f9df22ee"}},"ee23da2587174c5b9a051c4a288b0cf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c3baf2d3c854a03a57094384c1d5040","placeholder":"​","style":"IPY_MODEL_1b3332214534430790f186bc242f878b","value":"Downloading: 100%"}},"489a5866c4ee4174aa375d58c278d117":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16e5d570001348fd933600b4e0c06f38","max":466081,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ebc0ccf88a74da39863fed07c7cd511","value":466081}},"7a4a46aaba124d9e9161f1ee38b4e465":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_196433f9d4484419b0f01b698e6c23ec","placeholder":"​","style":"IPY_MODEL_c663ac9d686b413a8285b24cfe1e28f5","value":" 466k/466k [00:00&lt;00:00, 595kB/s]"}},"dd596aa7aa004ce88385cef1f9df22ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c3baf2d3c854a03a57094384c1d5040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b3332214534430790f186bc242f878b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16e5d570001348fd933600b4e0c06f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ebc0ccf88a74da39863fed07c7cd511":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"196433f9d4484419b0f01b698e6c23ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c663ac9d686b413a8285b24cfe1e28f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a11a48a7c6d4a4e907be68181eb1f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f6bf3368a9f4923871b047fc65b90db","IPY_MODEL_00f93992b34e4c22a623b472bc5b96fd","IPY_MODEL_5de6f28900f34676aa94ed7bbd30d5db"],"layout":"IPY_MODEL_38f39444fd254b4c97290c8a24e92faf"}},"6f6bf3368a9f4923871b047fc65b90db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17495c6b6afe44b298239dcc91994c06","placeholder":"​","style":"IPY_MODEL_5c3b89914e8449ea85ab6672322c68bc","value":"Downloading: 100%"}},"00f93992b34e4c22a623b472bc5b96fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f90de5bafae44c13b8f9ca316d0c663e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acb7eb5ea80641169db55a372146b913","value":2}},"5de6f28900f34676aa94ed7bbd30d5db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_572297c0d29640c78d0030b1e91b040f","placeholder":"​","style":"IPY_MODEL_005b400bdf864af0a2c270de96f3bb7d","value":" 2.00/2.00 [00:00&lt;00:00, 51.2B/s]"}},"38f39444fd254b4c97290c8a24e92faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17495c6b6afe44b298239dcc91994c06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c3b89914e8449ea85ab6672322c68bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f90de5bafae44c13b8f9ca316d0c663e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acb7eb5ea80641169db55a372146b913":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"572297c0d29640c78d0030b1e91b040f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"005b400bdf864af0a2c270de96f3bb7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a709e385014b44aeb99bb9211f7c6101":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6394aadf852b4fe0ab28cc9efe5652f5","IPY_MODEL_1256579b5e7c483a81db07a0bc45f1dd","IPY_MODEL_72aa900bc9a046959bc08a6d88f5803b"],"layout":"IPY_MODEL_a1d3be8d89344720adb1da71e7e6d14f"}},"6394aadf852b4fe0ab28cc9efe5652f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1754fe04eee14660b3d2a2624cb6ed6b","placeholder":"​","style":"IPY_MODEL_5c283f956c6446dba55089785c360a44","value":"Downloading: 100%"}},"1256579b5e7c483a81db07a0bc45f1dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18d03f04e86a4a16ba47c0887344fa46","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d26e62be38bb40d0aba8508dcb2a6b67","value":112}},"72aa900bc9a046959bc08a6d88f5803b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36a6f72446b948bba62c04be0ed3b921","placeholder":"​","style":"IPY_MODEL_dfbe305ec8534bd0a618484d072d2cff","value":" 112/112 [00:00&lt;00:00, 3.85kB/s]"}},"a1d3be8d89344720adb1da71e7e6d14f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1754fe04eee14660b3d2a2624cb6ed6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c283f956c6446dba55089785c360a44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18d03f04e86a4a16ba47c0887344fa46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d26e62be38bb40d0aba8508dcb2a6b67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36a6f72446b948bba62c04be0ed3b921":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfbe305ec8534bd0a618484d072d2cff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"059ed6693d014d3b9ab356c29101628f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9866f1fe0db3459ea00e5c13c666944e","IPY_MODEL_2d47b8843cd1497ab1be82c35287daa7","IPY_MODEL_ebb7d406672e4de394e9b488319d8937"],"layout":"IPY_MODEL_c04214d2193544f48122a72b56c58bf7"}},"9866f1fe0db3459ea00e5c13c666944e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_256b02e80eba472f88342a4fe38ddd1b","placeholder":"​","style":"IPY_MODEL_72ca6d2f6140491bb58d3c6c95d8e422","value":"Downloading: 100%"}},"2d47b8843cd1497ab1be82c35287daa7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f60de18aa074ee2a4580e790e5be2f8","max":409,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c1b93daca8a4e49b79763fbce290933","value":409}},"ebb7d406672e4de394e9b488319d8937":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12e2309a7ff744a6bc5389669c0ac034","placeholder":"​","style":"IPY_MODEL_90f36abe9501446395219c0fd5dd09dd","value":" 409/409 [00:00&lt;00:00, 11.2kB/s]"}},"c04214d2193544f48122a72b56c58bf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"256b02e80eba472f88342a4fe38ddd1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72ca6d2f6140491bb58d3c6c95d8e422":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f60de18aa074ee2a4580e790e5be2f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1b93daca8a4e49b79763fbce290933":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12e2309a7ff744a6bc5389669c0ac034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f36abe9501446395219c0fd5dd09dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0237b89a71d84587b714bc68885318cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2d0603715c54c009985896eba920a58","IPY_MODEL_63efa0cf09dc41a492a6d1f591ec3348","IPY_MODEL_f4eb1e00ad5943c7b6f51a32fd2b8784"],"layout":"IPY_MODEL_e1d9068daa1f4898bef61942a1c47988"}},"b2d0603715c54c009985896eba920a58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82148c20bbe44f0aa529a65c78c98447","placeholder":"​","style":"IPY_MODEL_07680e9df3c8433e93a4e143df023bee","value":"Downloading: 100%"}},"63efa0cf09dc41a492a6d1f591ec3348":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18525d8b228b4cc985363634fcd3b25d","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9ac6f6dff8448eeb107efc2e5dd21f2","value":625}},"f4eb1e00ad5943c7b6f51a32fd2b8784":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dac4895cd536454392d5be3eef324abf","placeholder":"​","style":"IPY_MODEL_6683a66d492e4a028a1566df491dc2da","value":" 625/625 [00:00&lt;00:00, 24.5kB/s]"}},"e1d9068daa1f4898bef61942a1c47988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82148c20bbe44f0aa529a65c78c98447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07680e9df3c8433e93a4e143df023bee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18525d8b228b4cc985363634fcd3b25d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ac6f6dff8448eeb107efc2e5dd21f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dac4895cd536454392d5be3eef324abf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6683a66d492e4a028a1566df491dc2da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd1d2607a4c64baca0b817c369ef649d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77be7c68bbef4ac6ab5ca09c4ba2de35","IPY_MODEL_f6bdf1f6480b413db8cd411aac1531cb","IPY_MODEL_b921fb231444426ba9ce0bc1325f1ac1"],"layout":"IPY_MODEL_0c57809441ad4ce3bde345262ed1a0d8"}},"77be7c68bbef4ac6ab5ca09c4ba2de35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8916b791c93040cd9f65d46b0a1c1f93","placeholder":"​","style":"IPY_MODEL_1a0835692a6a4cce888bdae21a8d093d","value":"Downloading: 100%"}},"f6bdf1f6480b413db8cd411aac1531cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2a9343c36144bd853aa671f1d9b2a9","max":438007537,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eab81106eec446aea60eff9181d10f85","value":438007537}},"b921fb231444426ba9ce0bc1325f1ac1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16663ff0566c4f2684440b3693a625f2","placeholder":"​","style":"IPY_MODEL_eb586c77888f4e4ba766354bf07239c5","value":" 438M/438M [00:07&lt;00:00, 64.3MB/s]"}},"0c57809441ad4ce3bde345262ed1a0d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8916b791c93040cd9f65d46b0a1c1f93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a0835692a6a4cce888bdae21a8d093d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c2a9343c36144bd853aa671f1d9b2a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab81106eec446aea60eff9181d10f85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16663ff0566c4f2684440b3693a625f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb586c77888f4e4ba766354bf07239c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"id":"3b816077","executionInfo":{"status":"ok","timestamp":1668824815525,"user_tz":300,"elapsed":5,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"id":"3b816077","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQcPZpEouvE2","executionInfo":{"status":"ok","timestamp":1668824839693,"user_tz":300,"elapsed":24173,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}},"outputId":"473fcfeb-8a6f-4fee-fa97-425ea944b6f9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"WQcPZpEouvE2","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUECyENeltWJ","executionInfo":{"status":"ok","timestamp":1668824884297,"user_tz":300,"elapsed":44606,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}},"outputId":"1ee1f596-919c-4481-82fc-131cb1f89720"},"source":["!pip install transformers==4.3.3\n","!pip install stop_words\n","!pip install symspellpy\n","!pip install language_detector \n","!pip install cached_property\n","!pip install sentencepiece\n","!pip install config\n","!pip install umap\n","!pip install sentence-transformers==0.4.1\n","!pip install nltk\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"id":"vUECyENeltWJ","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.3.3\n","  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 15.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (3.8.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (2022.6.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 77.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 83.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (4.13.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3) (3.10.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=3a66eccfbdfb0ce470bec6cc9ab54f2ee0d7ab2c296c89150d60b96a43fea2fe\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.3.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting stop_words\n","  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n","Building wheels for collected packages: stop-words\n","  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32912 sha256=3c87c2e3edd06a091085c9c6eace95b56f9e107a11bce656e059b4744a35edf3\n","  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n","Successfully built stop-words\n","Installing collected packages: stop-words\n","Successfully installed stop-words-2018.7.23\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting symspellpy\n","  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 14.7 MB/s \n","\u001b[?25hCollecting editdistpy>=0.1.3\n","  Downloading editdistpy-0.1.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 82.8 MB/s \n","\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n","Successfully installed editdistpy-0.1.3 symspellpy-6.7.7\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting language_detector\n","  Downloading language-detector-5.0.2.tar.gz (6.6 kB)\n","Building wheels for collected packages: language-detector\n","  Building wheel for language-detector (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for language-detector: filename=language_detector-5.0.2-py3-none-any.whl size=7054 sha256=cf95b5190bb457560c78989904880c44094d10b128510ac897a8bb8e9f0f38b9\n","  Stored in directory: /root/.cache/pip/wheels/12/40/73/a0765d65e793332b79dfe6c34c713e7c0066ea785191b3f50a\n","Successfully built language-detector\n","Installing collected packages: language-detector\n","Successfully installed language-detector-5.0.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cached_property in /usr/local/lib/python3.7/dist-packages (1.5.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 15.3 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting config\n","  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n","Installing collected packages: config\n","Successfully installed config-0.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting umap\n","  Downloading umap-0.1.1.tar.gz (3.2 kB)\n","Building wheels for collected packages: umap\n","  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3565 sha256=41ccafcc7d387f5849f3cecf9958e8e988d2de3a72617d84c58c6c8d76c27095\n","  Stored in directory: /root/.cache/pip/wheels/65/55/85/945cfb3d67373767e4dc3e9629300a926edde52633df4f0efe\n","Successfully built umap\n","Installing collected packages: umap\n","Successfully installed umap-0.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence-transformers==0.4.1\n","  Downloading sentence-transformers-0.4.1.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (4.3.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (4.64.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (1.12.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (3.7)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.4.1) (0.1.97)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers==0.4.1) (4.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (3.8.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (0.0.53)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (4.13.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (2022.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (3.10.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.4.1) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.4.1) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers==0.4.1) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.4.1) (3.1.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1-py3-none-any.whl size=103052 sha256=6d08c52f6d5babab8fed42951ba2cc553e2d235f331f4c90a40bb776847c6043\n","  Stored in directory: /root/.cache/pip/wheels/ed/2b/00/349b245f5256ba7a5d874bcb0f1f2dfc8b4b963d80b38626ac\n","Successfully built sentence-transformers\n","Installing collected packages: sentence-transformers\n","Successfully installed sentence-transformers-0.4.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7yHUDTQ13BL","executionInfo":{"status":"ok","timestamp":1668824889098,"user_tz":300,"elapsed":4810,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}},"outputId":"c8a45619-bc98-4dd3-c220-7d66acf1c4b3"},"source":["import tensorflow, sentence_transformers, transformers\n","tensorflow.__version__, sentence_transformers.__version__, transformers.__version__"],"id":"J7yHUDTQ13BL","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('2.9.2', '0.4.1', '4.3.3')"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"nzgNkHqtuw_A","executionInfo":{"status":"ok","timestamp":1668824889098,"user_tz":300,"elapsed":2,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":["root_path = \"/content/drive/MyDrive/SCHOOL/PhD/Code/context-encoder-v2\"\n","import sys, os\n","import config\n","\n","config.root_path = os.path.abspath(root_path)\n","sys.path.insert(0, config.root_path)"],"id":"nzgNkHqtuw_A","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"811f4ff7","executionInfo":{"status":"ok","timestamp":1668824918569,"user_tz":300,"elapsed":29472,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}},"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["7eb102530b70401b86763598ab4f301b","b3cb2c9f0e914f0e87926914de405303","dd564ebfde7c4968acab7f1c3f7c0566","be6f187690bf4008b41b86806d7e4a1d","b1d9a500880a4aa1904b36f11fab8941","42d0690b09e54f42a25b21a5a5f02838","dfc534f8a8a740c69925cde6e5ba9b9b","b3adde2b77054bc584a5ee0b8ad9a8ce","39101fbb9e9f48dda4c4525274c37bce","fb2669bed87b47f39ae14d5b22d4eed3","62b90f280395404f80ca07e351baaef8","c33ebaf403084905a8312dbcc63f5003","b540b483b1d244e7847d05338335e5ac","4081629bc2d34192863bc5b1f735ae29","72c35411cf2941dbbdcaca1f8f30b849","33b88e5ccd3d47dca6ded7e418095884","dfdeba944b5542dd806779cdf00db68c","b656607da5b74226b3fd7b7abf921d35","8d7e4f9b29a24088ba1d976a5bd321ac","29f0d28e130e4a4883a6461756ffe0c2","86d63101b99c40ce8d5d0f97a33703c7","ff5ae31ae002433a982e4cc4cf389244","480cfc1835cd41cf994ee07a0258cd25","ee23da2587174c5b9a051c4a288b0cf1","489a5866c4ee4174aa375d58c278d117","7a4a46aaba124d9e9161f1ee38b4e465","dd596aa7aa004ce88385cef1f9df22ee","5c3baf2d3c854a03a57094384c1d5040","1b3332214534430790f186bc242f878b","16e5d570001348fd933600b4e0c06f38","0ebc0ccf88a74da39863fed07c7cd511","196433f9d4484419b0f01b698e6c23ec","c663ac9d686b413a8285b24cfe1e28f5","4a11a48a7c6d4a4e907be68181eb1f3b","6f6bf3368a9f4923871b047fc65b90db","00f93992b34e4c22a623b472bc5b96fd","5de6f28900f34676aa94ed7bbd30d5db","38f39444fd254b4c97290c8a24e92faf","17495c6b6afe44b298239dcc91994c06","5c3b89914e8449ea85ab6672322c68bc","f90de5bafae44c13b8f9ca316d0c663e","acb7eb5ea80641169db55a372146b913","572297c0d29640c78d0030b1e91b040f","005b400bdf864af0a2c270de96f3bb7d","a709e385014b44aeb99bb9211f7c6101","6394aadf852b4fe0ab28cc9efe5652f5","1256579b5e7c483a81db07a0bc45f1dd","72aa900bc9a046959bc08a6d88f5803b","a1d3be8d89344720adb1da71e7e6d14f","1754fe04eee14660b3d2a2624cb6ed6b","5c283f956c6446dba55089785c360a44","18d03f04e86a4a16ba47c0887344fa46","d26e62be38bb40d0aba8508dcb2a6b67","36a6f72446b948bba62c04be0ed3b921","dfbe305ec8534bd0a618484d072d2cff","059ed6693d014d3b9ab356c29101628f","9866f1fe0db3459ea00e5c13c666944e","2d47b8843cd1497ab1be82c35287daa7","ebb7d406672e4de394e9b488319d8937","c04214d2193544f48122a72b56c58bf7","256b02e80eba472f88342a4fe38ddd1b","72ca6d2f6140491bb58d3c6c95d8e422","1f60de18aa074ee2a4580e790e5be2f8","1c1b93daca8a4e49b79763fbce290933","12e2309a7ff744a6bc5389669c0ac034","90f36abe9501446395219c0fd5dd09dd"]},"outputId":"8575da7d-65ec-4c83-a175-435b9095a452"},"source":["import tensorflow as tf\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","from src.encoders.context_encoder_ldabert_2 import ContextEncoderComplex, ContextEncoderConv\n","from src.dataset.ldabert_2 import LDABERT2Dataset\n","\n","from tensorflow.python import keras\n","import toml\n","import json\n","import pandas as pd\n","import numpy as np\n","from utils.experiments import get_experiments_json, get_experiments, save_results"],"id":"811f4ff7","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/630 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb102530b70401b86763598ab4f301b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33ebaf403084905a8312dbcc63f5003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"480cfc1835cd41cf994ee07a0258cd25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a11a48a7c6d4a4e907be68181eb1f3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a709e385014b44aeb99bb9211f7c6101"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/409 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"059ed6693d014d3b9ab356c29101628f"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"26986a31","executionInfo":{"status":"ok","timestamp":1668824918569,"user_tz":300,"elapsed":11,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":[],"id":"26986a31","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"5a5de02f","executionInfo":{"status":"ok","timestamp":1668824918571,"user_tz":300,"elapsed":12,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":[],"id":"5a5de02f","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66caca4d"},"source":["## Experiment"],"id":"66caca4d"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"gTJTmy-X1gyN","executionInfo":{"status":"ok","timestamp":1668824919354,"user_tz":300,"elapsed":794,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}},"outputId":"fe79cf4b-fdf9-4ee9-e2c4-ac0a2b39d594"},"source":["experiments_config = get_experiments_json('ldabert2_complex_finetune_low_epochs_test')\n","experiments_config_df = pd.DataFrame.from_dict(experiments_config)\n","experiments_config_df"],"id":"gTJTmy-X1gyN","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  bert_type dataset_type  final_dropout  dense_neurons  max_sentence_length  \\\n","0   ldabert     clinical           0.25            256                  128   \n","1   ldabert         wiki           0.25            256                  128   \n","2   ldabert      fiction           0.25            256                  128   \n","\n","   lstm_size  lstm_dropout_percentage  cnn_filters  cnn_kernel_size  \\\n","0        256                     0.25            8                3   \n","1        256                     0.25            8                3   \n","2        256                     0.25            8                3   \n","\n","   pool_size  gamma  pct_data  augment_pct  bert_trainable  epochs  \n","0          2     15         1            1            True      10  \n","1          2     15         1            1            True      10  \n","2          2     15         1            1            True      10  "],"text/html":["\n","  <div id=\"df-1feb46aa-c89b-4605-a0fe-d826ee7f2226\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bert_type</th>\n","      <th>dataset_type</th>\n","      <th>final_dropout</th>\n","      <th>dense_neurons</th>\n","      <th>max_sentence_length</th>\n","      <th>lstm_size</th>\n","      <th>lstm_dropout_percentage</th>\n","      <th>cnn_filters</th>\n","      <th>cnn_kernel_size</th>\n","      <th>pool_size</th>\n","      <th>gamma</th>\n","      <th>pct_data</th>\n","      <th>augment_pct</th>\n","      <th>bert_trainable</th>\n","      <th>epochs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ldabert</td>\n","      <td>clinical</td>\n","      <td>0.25</td>\n","      <td>256</td>\n","      <td>128</td>\n","      <td>256</td>\n","      <td>0.25</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ldabert</td>\n","      <td>wiki</td>\n","      <td>0.25</td>\n","      <td>256</td>\n","      <td>128</td>\n","      <td>256</td>\n","      <td>0.25</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ldabert</td>\n","      <td>fiction</td>\n","      <td>0.25</td>\n","      <td>256</td>\n","      <td>128</td>\n","      <td>256</td>\n","      <td>0.25</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1feb46aa-c89b-4605-a0fe-d826ee7f2226')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1feb46aa-c89b-4605-a0fe-d826ee7f2226 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1feb46aa-c89b-4605-a0fe-d826ee7f2226');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"p-JZ0fAd1oBO","executionInfo":{"status":"ok","timestamp":1668824919665,"user_tz":300,"elapsed":313,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":["experiments_config_df.to_csv(r'../models/experiment.csv', header=None, index=None, sep=' ', mode='a')"],"id":"p-JZ0fAd1oBO","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtQh12191ome","executionInfo":{"status":"ok","timestamp":1668824919666,"user_tz":300,"elapsed":3,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":["def get_random_hash(k):\n","  import random, string\n","  x = ''.join(random.choices(string.ascii_letters + string.digits, k=k))\n","  return x"],"id":"rtQh12191ome","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJXQb_X7ymh1","executionInfo":{"status":"ok","timestamp":1668824919666,"user_tz":300,"elapsed":2,"user":{"displayName":"Amit Maraj","userId":"07292138418067546452"}}},"source":[],"id":"iJXQb_X7ymh1","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0237b89a71d84587b714bc68885318cc","b2d0603715c54c009985896eba920a58","63efa0cf09dc41a492a6d1f591ec3348","f4eb1e00ad5943c7b6f51a32fd2b8784","e1d9068daa1f4898bef61942a1c47988","82148c20bbe44f0aa529a65c78c98447","07680e9df3c8433e93a4e143df023bee","18525d8b228b4cc985363634fcd3b25d","f9ac6f6dff8448eeb107efc2e5dd21f2","dac4895cd536454392d5be3eef324abf","6683a66d492e4a028a1566df491dc2da","fd1d2607a4c64baca0b817c369ef649d","77be7c68bbef4ac6ab5ca09c4ba2de35","f6bdf1f6480b413db8cd411aac1531cb","b921fb231444426ba9ce0bc1325f1ac1","0c57809441ad4ce3bde345262ed1a0d8","8916b791c93040cd9f65d46b0a1c1f93","1a0835692a6a4cce888bdae21a8d093d","5c2a9343c36144bd853aa671f1d9b2a9","eab81106eec446aea60eff9181d10f85","16663ff0566c4f2684440b3693a625f2","eb586c77888f4e4ba766354bf07239c5"]},"id":"a8659ace","outputId":"0ae1cb07-bf6e-4e7e-f090-88609e88c1de"},"source":["for experiment in experiments_config[0:3]: # all\n","    dataset_type = experiment['dataset_type']\n","    final_dropout = experiment['final_dropout']\n","    dense_neurons = experiment['dense_neurons']\n","    lstm_size = experiment['lstm_size']\n","    lstm_dropout_percentage = experiment['lstm_dropout_percentage']\n","    cnn_filters = experiment['cnn_filters']\n","    cnn_kernel_size = experiment['cnn_kernel_size']\n","    pool_size = experiment['pool_size']\n","    \n","    pct_data = experiment['pct_data']\n","    augment_pct = experiment['augment_pct']\n","    gamma = experiment['gamma']\n","    max_sentence_length = experiment['max_sentence_length']\n","    bert_trainable = experiment['bert_trainable']\n","    finetuning_epochs = 5 # finetune the system for 5 epochs before training with the frozen finetuned BERT weights\n","    epochs = experiment['epochs']\n","    BATCH_SIZE = 64\n","    print(\"params:\", experiment)\n","    random_hash = get_random_hash(5)\n","    experiment['epochs'] = epochs\n","\n","    for step in [\"finetune\",\"frozen\"]:\n","    # for step in [\"frozen\"]:\n","    \n","      # init model\n","      print(\"initializing model...\")\n","      model = ContextEncoderComplex(final_dropout=final_dropout,\n","                              dense_neurons=dense_neurons,\n","                                    lstm_size=lstm_size,\n","                              lstm_dropout_percentage=lstm_dropout_percentage,\n","                              cnn_filters=cnn_filters,\n","                              cnn_kernel_size=cnn_kernel_size,\n","                              pool_size=pool_size,\n","                            gamma=gamma,\n","                            max_sentence_length=max_sentence_length,\n","                            bert_trainable=True if step == \"finetune\" else False)\n","      \n","      # print(\"number of params: \", sum([np.prod(keras.get_value(w).shape) for w in model.trainable_weights]))\n","      \n","      # init dataset\n","      print(\"initializing dataset...\")\n","      dataset = LDABERT2Dataset(dataset_type=dataset_type,\n","                              pct_data=pct_data,\n","                            max_seq_length=max_sentence_length,\n","                              max_segment_length=5,\n","                              augment_pct=augment_pct,\n","                              split=\"train\",\n","                        artificial_segments=True)\n","      \n","      # process dataset\n","      print(\"processing dataset...\")\n","      sentences, tokenized_sentences, labels = dataset.process()\n","\n","      vectors_filename = '{}_{}_as-{}_msl-{}.pkl'.format(dataset.pct_data, dataset.augment_pct, dataset.artificial_segments, dataset.max_segment_length)\n","\n","      saved_vectors, saved_labels, saved_sentences, saved_tokenized_sentences = dataset.get_saved_vectors(\"train\", dataset.dataset_type, vectors_filename)\n","\n","      if len(saved_vectors) == 0:\n","          saved_vectors, saved_labels, saved_sentences, saved_tokenized_sentences = dataset.create_vectors(\"train\", dataset.dataset_type, vectors_filename)\n","\n","      left_input, mid_input, right_input = dataset.format_sentences_tri_input_plus(saved_tokenized_sentences)\n","      lda_left_input, lda_mid_input, lda_right_input = dataset.format_sentences_tri_input(saved_vectors)\n","\n","      # get class weight\n","      neg, pos = np.bincount(labels.flatten())\n","      initial_bias = np.log([pos/neg])\n","      \n","      total=len(labels)\n","      weight_for_0 = (1 / neg)*(total)/2.0 \n","      weight_for_1 = (1 / pos)*(total)/2.0\n","\n","      class_weight = {0: weight_for_0, 1: weight_for_1}\n","      print(\"class weight\", class_weight)\n","      \n","      # create checkpoint path\n","      checkpoint_filepath = '{}/models/LDABERT2/hybrid-conv/{}-{}-{}-pct-{}-aug_{}/finetune-{}/checkpoint'.format(\n","                              config.root_path,\n","                              dataset.dataset_type,                    \n","                              len(saved_sentences), \n","                              dataset.pct_data,\n","                              dataset.augment_pct,\n","                              random_hash,\n","                              bert_trainable)\n","      \n","      # continue training\n","      if step == \"frozen\":\n","        # random_hash = 'WLZ0q'\n","        checkpoint_filepath = '{}/models/LDABERT2/hybrid-conv/{}-{}-{}-pct-{}-aug_{}/finetune-{}/checkpoint'.format(\n","                              config.root_path,\n","                              dataset.dataset_type,                    \n","                              len(saved_sentences), \n","                              dataset.pct_data,\n","                              dataset.augment_pct,\n","                              random_hash,\n","                              bert_trainable)\n","      \n","      print(checkpoint_filepath)\n","      \n","      # get callbacks ready.\n","      model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","          filepath=checkpoint_filepath,\n","          save_weights_only=False,\n","          monitor='val_accuracy',\n","          save_best_only=True,\n","          mode=\"auto\",\n","          save_freq=\"epoch\")\n","\n","      early_stopping = tf.keras.callbacks.EarlyStopping(\n","          monitor='val_accuracy', \n","          verbose=1,\n","          patience=10,\n","          mode='max',\n","          restore_best_weights=True)\n","      \n","      log_path = '{}/models/LDABERT2/hybrid-conv/{}-{}-{}-pct-{}-aug_{}/finetune-{}/checkpoint/training.log'.format(\n","                              config.root_path,\n","                              dataset.dataset_type,                    \n","                              len(saved_sentences), \n","                              dataset.pct_data,\n","                              dataset.augment_pct,\n","                              random_hash,\n","                              bert_trainable)\n","\n","      csv_logger = tf.keras.callbacks.CSVLogger(log_path, separator=\",\", append=False)\n","\n","      callbacks = [\n","      #     early_stopping,\n","          model_checkpoint_callback,\n","          csv_logger\n","      ]\n","      \n","      # compiling model\n","      print(\"compiling the model...\")\n","      model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","                    loss=tf.keras.losses.BinaryCrossentropy(),\n","                    metrics=[\n","                        keras.metrics.BinaryAccuracy(name='accuracy')\n","                    ])\n","      \n","      try:\n","          model.load_weights(checkpoint_filepath)\n","          print(\"model loaded.\")\n","      except:\n","          print(\"No checkpoint available.\")\n","      \n","      print(\"starting the training process...\")\n","      # remove warnings\n","      tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","      history = model.fit([\n","                              left_input, mid_input, right_input, \n","                              lda_left_input, lda_mid_input, lda_right_input\n","                          ], \n","                          tf.convert_to_tensor(saved_labels), \n","                          epochs=finetuning_epochs if step == \"finetune\" else epochs,\n","                          validation_split=0.25,\n","                          batch_size=BATCH_SIZE,\n","                          verbose=1, \n","                          class_weight=class_weight,\n","                          callbacks=callbacks)\n","\n","      # assigning history to experiment object for saving.\n","      experiment[\"history\"] = history.history\n","      experiment[\"hash\"] = random_hash\n","      \n","      print(\"saving results...\")\n","      save_results(experiment)"],"id":"a8659ace","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["params: {'bert_type': 'ldabert', 'dataset_type': 'clinical', 'final_dropout': 0.25, 'dense_neurons': 256, 'max_sentence_length': 128, 'lstm_size': 256, 'lstm_dropout_percentage': 0.25, 'cnn_filters': 8, 'cnn_kernel_size': 3, 'pool_size': 2, 'gamma': 15, 'pct_data': 1, 'augment_pct': 1, 'bert_trainable': True, 'epochs': 10}\n","initializing model...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0237b89a71d84587b714bc68885318cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1d2607a4c64baca0b817c369ef649d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["initializing dataset...\n","processing dataset...\n","artificial segments True\n","artificial segments True\n","class weight {0: 0.6484113712374582, 1: 2.184507042253521}\n","/content/drive/MyDrive/SCHOOL/PhD/Code/context-encoder-v2/models/LDABERT2/hybrid-conv/clinical-44979-1-pct-1-aug_gZLlY/finetune-True/checkpoint\n","compiling the model...\n","No checkpoint available.\n","starting the training process...\n","Epoch 1/5\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","528/528 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.6266sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 462s 771ms/step - loss: 0.6853 - accuracy: 0.6266 - val_loss: 0.6633 - val_accuracy: 0.6485\n","Epoch 2/5\n","528/528 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.6799sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 403s 764ms/step - loss: 0.6852 - accuracy: 0.6799 - val_loss: 0.6876 - val_accuracy: 0.7011\n","Epoch 3/5\n","528/528 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.7092sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 401s 760ms/step - loss: 0.6851 - accuracy: 0.7092 - val_loss: 0.6903 - val_accuracy: 0.7107\n","Epoch 4/5\n","528/528 [==============================] - 330s 624ms/step - loss: 0.6849 - accuracy: 0.7049 - val_loss: 0.6699 - val_accuracy: 0.7087\n","Epoch 5/5\n","528/528 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.7086sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex/tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 402s 762ms/step - loss: 0.6851 - accuracy: 0.7086 - val_loss: 0.6716 - val_accuracy: 0.7130\n","saving results...\n","initializing model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["initializing dataset...\n","processing dataset...\n","artificial segments True\n","artificial segments True\n","class weight {0: 0.6484113712374582, 1: 2.184507042253521}\n","/content/drive/MyDrive/SCHOOL/PhD/Code/context-encoder-v2/models/LDABERT2/hybrid-conv/clinical-44979-1-pct-1-aug_gZLlY/finetune-True/checkpoint\n","compiling the model...\n","model loaded.\n","starting the training process...\n","Epoch 1/10\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","527/528 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.6768sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 257s 422ms/step - loss: 0.6850 - accuracy: 0.6768 - val_loss: 0.6558 - val_accuracy: 0.7068\n","Epoch 2/10\n","527/528 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.7185sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 214s 406ms/step - loss: 0.6851 - accuracy: 0.7185 - val_loss: 0.6801 - val_accuracy: 0.7305\n","Epoch 3/10\n","527/528 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.7351sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 217s 410ms/step - loss: 0.6850 - accuracy: 0.7351 - val_loss: 0.6864 - val_accuracy: 0.7309\n","Epoch 4/10\n","527/528 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.7298sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 214s 406ms/step - loss: 0.6851 - accuracy: 0.7298 - val_loss: 0.6862 - val_accuracy: 0.7321\n","Epoch 5/10\n","528/528 [==============================] - 142s 269ms/step - loss: 0.6850 - accuracy: 0.7319 - val_loss: 0.6891 - val_accuracy: 0.7309\n","Epoch 6/10\n","527/528 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.7333sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 215s 408ms/step - loss: 0.6851 - accuracy: 0.7333 - val_loss: 0.6854 - val_accuracy: 0.7340\n","Epoch 7/10\n","527/528 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.7361sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 216s 409ms/step - loss: 0.6849 - accuracy: 0.7361 - val_loss: 0.6877 - val_accuracy: 0.7371\n","Epoch 8/10\n","528/528 [==============================] - 142s 269ms/step - loss: 0.6850 - accuracy: 0.7366 - val_loss: 0.6857 - val_accuracy: 0.7359\n","Epoch 9/10\n","527/528 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.7379sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_1/tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_1/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_1/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r528/528 [==============================] - 214s 406ms/step - loss: 0.6850 - accuracy: 0.7379 - val_loss: 0.6874 - val_accuracy: 0.7374\n","Epoch 10/10\n","528/528 [==============================] - 142s 269ms/step - loss: 0.6849 - accuracy: 0.7358 - val_loss: 0.6909 - val_accuracy: 0.7337\n","saving results...\n","params: {'bert_type': 'ldabert', 'dataset_type': 'wiki', 'final_dropout': 0.25, 'dense_neurons': 256, 'max_sentence_length': 128, 'lstm_size': 256, 'lstm_dropout_percentage': 0.25, 'cnn_filters': 8, 'cnn_kernel_size': 3, 'pool_size': 2, 'gamma': 15, 'pct_data': 1, 'augment_pct': 1, 'bert_trainable': True, 'epochs': 10}\n","initializing model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["initializing dataset...\n","processing dataset...\n","artificial segments True\n","artificial segments True\n","class weight {0: 0.6378117432462183, 1: 2.3140689183021452}\n","/content/drive/MyDrive/SCHOOL/PhD/Code/context-encoder-v2/models/LDABERT2/hybrid-conv/wiki-40561-1-pct-1-aug_iRZW8/finetune-True/checkpoint\n","compiling the model...\n","No checkpoint available.\n","starting the training process...\n","Epoch 1/5\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","476/476 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4921sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'context_encoder_complex_2/tf_bert_model_2/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_2/StatefulPartitionedCall:1' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_2/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n","sbert_output TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor 'tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0' shape=(None, 128, 768) dtype=float32>, pooler_output=<tf.Tensor 'tf_bert_model_2/bert/pooler/dense/Tanh:0' shape=(None, 768) dtype=float32>, hidden_states=None, attentions=None)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 443). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r476/476 [==============================] - 418s 789ms/step - loss: 0.6941 - accuracy: 0.4921 - val_loss: 0.6831 - val_accuracy: 0.5727\n","Epoch 2/5\n","476/476 [==============================] - 297s 624ms/step - loss: 0.6937 - accuracy: 0.5677 - val_loss: 0.6876 - val_accuracy: 0.5719\n","Epoch 3/5\n","476/476 [==============================] - 297s 624ms/step - loss: 0.6936 - accuracy: 0.5739 - val_loss: 0.6883 - val_accuracy: 0.5693\n","Epoch 4/5\n","476/476 [==============================] - 297s 624ms/step - loss: 0.6936 - accuracy: 0.5802 - val_loss: 0.7053 - val_accuracy: 0.5510\n","Epoch 5/5\n","476/476 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5392"]}]},{"cell_type":"code","metadata":{"id":"fyRFxeEbxcmw"},"source":["from matplotlib import pyplot as plt\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"id":"fyRFxeEbxcmw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sVFESlTpQ18"},"source":[],"id":"2sVFESlTpQ18","execution_count":null,"outputs":[]}]}