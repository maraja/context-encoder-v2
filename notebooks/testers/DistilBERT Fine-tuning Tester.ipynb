{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b816077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb492ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from src.encoders.context_encoder_bert import ContextEncoder\n",
    "\n",
    "from tensorflow.python import keras\n",
    "import toml\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from src.dataset.distilbert import DistilBERTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f8a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALBERT_FINETUNE_SIMPLE': [{'bert_type': 'albert', 'dataset_type': 'clinical', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 1, 'epochs': 200}, {'bert_type': 'albert', 'dataset_type': 'fiction', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 1, 'epochs': 200}, {'bert_type': 'albert', 'dataset_type': 'wiki', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 200}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 1, 'epochs': 200}], 'ALBERT_FINETUNE_COMPLEX': {'foo': 'bar'}, 'LDA_BERT_FINETUNE_SIMPLE': {'foo': 'bar'}, 'ALBERT_FINETUNE_TEST': [{'bert_type': 'albert', 'dataset_type': 'clinical', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'albert', 'dataset_type': 'fiction', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'albert', 'dataset_type': 'wiki', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'albert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}], 'DISTILBERT_FINETUNE_TEST': [{'bert_type': 'distilbert', 'dataset_type': 'clinical', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'clinical', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'distilbert', 'dataset_type': 'fiction', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'fiction', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}, {'bert_type': 'distilbert', 'dataset_type': 'wiki', 'finetune_bert': True, 'pct_data': 1, 'augment_pct': 0.1, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 0.5, 'epochs': 100}, {'bert_type': 'distilbert', 'finetune_bert': True, 'dataset_type': 'wiki', 'pct_data': 1, 'augment_pct': 1, 'epochs': 100}]}\n"
     ]
    }
   ],
   "source": [
    "# Read local `config.toml` file.\n",
    "config = toml.load('../settings/experiments.toml')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe96882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269c6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf55022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "# model = TFDistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "# input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\"))[None, :]  # Batch size 1\n",
    "# outputs = model(input_ids)\n",
    "# last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5be09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9beac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pool_output(input_tensor):\n",
    "#     bert_full_output = tf.transpose(input_tensor, [0, 2, 1])\n",
    "#     bert_pooled_output = tf.reduce_mean(bert_full_output, 2)\n",
    "#     return bert_pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846a3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_output(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb472e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe57803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f45a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContextEncoder(final_dropout=0.5,\n",
    "                       dense_neurons=64,\n",
    "                       bert_trainable=True,\n",
    "                       bert_type=\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1614dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_output = model(tf.constant([[[1,2]],[[3,4]],[[5,6]]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937c9530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.3498583]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde06f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  65190912  \n",
      "_________________________________________________________________\n",
      "dense_input_left (Dense)     multiple                  49216     \n",
      "_________________________________________________________________\n",
      "dense_input_mid (Dense)      multiple                  49216     \n",
      "_________________________________________________________________\n",
      "dense_input_right (Dense)    multiple                  49216     \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         multiple                  193       \n",
      "_________________________________________________________________\n",
      "final_dropout (Dropout)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 65,338,753\n",
      "Trainable params: 65,338,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5575373",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DistilBERTDataset(dataset_type=\"clinical\",\n",
    "                       pct_data=0.1,\n",
    "                       max_segment_length=5,\n",
    "                        max_seq_length=128\n",
    "                       augment_pct=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef873bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer\\anaconda3\\envs\\phd\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "sentences, tokenized_sentences, labels = dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3c58eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1619, 128), dtype=int32, numpy=\n",
       " array([[  101,  1996,  2087, ..., 24759,  3022,   102],\n",
       "        [  101,  4998,  2013, ...,     0,     0,     0],\n",
       "        [  101,  1996,  2190, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  6064,  7192, ...,     0,     0,     0],\n",
       "        [  101,  1999, 25714, ...,     0,     0,     0],\n",
       "        [  101,  1059, 21030, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(1619, 128), dtype=int32, numpy=\n",
       " array([[  101,  4998,  2013, ...,     0,     0,     0],\n",
       "        [  101,  1996,  2190, ...,     0,     0,     0],\n",
       "        [  101,  1996,  5776, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1999, 25714, ...,     0,     0,     0],\n",
       "        [  101,  1059, 21030, ...,     0,     0,     0],\n",
       "        [  101,  1996,  2087, ..., 24759,  3022,   102]])>,\n",
       " <tf.Tensor: shape=(1619, 128), dtype=int32, numpy=\n",
       " array([[  101,  1996,  2190, ...,     0,     0,     0],\n",
       "        [  101,  1996,  5776, ...,     0,     0,     0],\n",
       "        [  101,  5022,  2323, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1059, 21030, ...,     0,     0,     0],\n",
       "        [  101,  1996,  2087, ..., 24759,  3022,   102],\n",
       "        [  101,  4998,  2013, ...,     0,     0,     0]])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.format_sentences_tri_input(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf73ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be4d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee53a13",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66085440",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d675b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "dense_output = 128\n",
    "\n",
    "# balanced = balanced binary crossentropy\n",
    "checkpoint_filepath = '../models/DistilBERT/finetune/simple/{}-{}-{}-pct-{}-aug/checkpoint'.format(\n",
    "                        dataset.dataset_type,                    \n",
    "                        len(sentences), \n",
    "                        dataset.pct_data,\n",
    "                        dataset.augment_pct)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "#     early_stopping,\n",
    "    model_checkpoint_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f9dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/DistilBERT/finetune/simple/clinical-1605-0.1-pct-0.1-aug/checkpoint'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c58221d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d303600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"No checkpoint available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e578f2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ContextEncoder.call of <src.encoders.context_encoder_bert.ContextEncoder object at 0x000001AD95F73208>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <bound method ContextEncoder.call of <src.encoders.context_encoder_bert.ContextEncoder object at 0x000001AD95F73208>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ContextEncoder.call of <src.encoders.context_encoder_bert.ContextEncoder object at 0x000001AD95F73208>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <bound method ContextEncoder.call of <src.encoders.context_encoder_bert.ContextEncoder object at 0x000001AD95F73208>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 64/361 [====>.........................] - ETA: 21:02 - loss: 0.9079 - accuracy: 0.7312"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5cb0686a9dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#                     class_weight=class_weight,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.format_sentences_tri_input(tokenized_sentences), \n",
    "                    tf.convert_to_tensor(labels), \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1, \n",
    "#                     class_weight=class_weight,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26986a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5de02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66caca4d",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de19ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from utils.experiments import get_experiments, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9b44d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_type</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>finetune_bert</th>\n",
       "      <th>pct_data</th>\n",
       "      <th>augment_pct</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bert_type dataset_type  finetune_bert  pct_data  augment_pct  epochs\n",
       "0    albert     clinical           True         1          0.1     200\n",
       "1    albert     clinical           True         1          0.5     200\n",
       "2    albert     clinical           True         1          1.0     200\n",
       "3    albert      fiction           True         1          0.1     200\n",
       "4    albert      fiction           True         1          0.5     200\n",
       "5    albert      fiction           True         1          1.0     200\n",
       "6    albert         wiki           True         1          0.1     200\n",
       "7    albert         wiki           True         1          0.5     200\n",
       "8    albert         wiki           True         1          1.0     200"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read local `config.toml` file.\n",
    "config = get_experiments('ALBERT_FINETUNE_SIMPLE')\n",
    "config_df = pd.DataFrame.from_dict(config)\n",
    "config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90091fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df.to_csv(r'../models/experiment.csv', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32004722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8659ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for experiment in config:\n",
    "    bert_type = config['bert_type']\n",
    "    dataset_type = config['dataset_type']\n",
    "    finetune_bert = config['finetune_bert']\n",
    "    pct_data = config['pct_data']\n",
    "    augment_pct = config['augment_pct']\n",
    "    epochs = config['epochs']\n",
    "    print(\"params:\", bert_type, dataset_type, finetune_bert, pct_data, augment_pct, epochs)\n",
    "    \n",
    "    # init model\n",
    "    print(\"initializing model...\")\n",
    "    model = ContextEncoder(final_dropout=0.5,\n",
    "                           dense_neurons=64,\n",
    "                           bert_trainable=finetune_bert,\n",
    "                           bert_type=\"albert-base-v2\")\n",
    "    \n",
    "    # init dataset\n",
    "    print(\"initializing dataset...\")\n",
    "    dataset = AlbertDataset(dataset_type=dataset_type,\n",
    "                           pct_data=pct_data,\n",
    "                           max_segment_length=5,\n",
    "                           augment_pct=augment_pct)\n",
    "    \n",
    "    # process dataset\n",
    "    print(\"processing dataset...\")\n",
    "    sentences, tokenized_sentences, labels = dataset.process()\n",
    "    \n",
    "    # create checkpoint path\n",
    "    checkpoint_filepath = '../models/ALBERT/finetune/simple/{}-{}-{}-pct-{}-aug/checkpoint'.format(\n",
    "                            dataset_type,                    \n",
    "                            len(sentences), \n",
    "                            pct_data,\n",
    "                            augment_pct)\n",
    "    print(checkpoint_filepath)\n",
    "    \n",
    "    # compiling model\n",
    "    print(\"compiling the model...\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    try:\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        print(\"model loaded.\")\n",
    "    except:\n",
    "        print(\"No checkpoint available.\")\n",
    "    \n",
    "    # \n",
    "    print(\"starting the training process...\")\n",
    "    history = model.fit(dataset.format_sentences_tri_input(tokenized_sentences), \n",
    "                        tf.convert_to_tensor(labels), \n",
    "                        epochs=EPOCHS,\n",
    "                        validation_split=0.1,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=1, \n",
    "                        # class_weight=class_weight,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    # assigning history to experiment object for saving.\n",
    "    experiment[\"history\"] = history\n",
    "    \n",
    "    print(\"saving results...\")\n",
    "    save_results(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36500af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd] *",
   "language": "python",
   "name": "conda-env-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
