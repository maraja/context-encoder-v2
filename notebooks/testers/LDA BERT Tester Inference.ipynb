{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b816077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e50641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb492ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.insert(0, config.root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python import keras\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.encoders.context_encoder_ldabert_2 import ContextEncoder\n",
    "from src.dataset.ldabert_2 import LDABERT2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7808d62a",
   "metadata": {},
   "source": [
    "## LDA BERT 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2891d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"clinical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a240f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\clinical\\\\0.005_0.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 130\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 130\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 130 130 130\n"
     ]
    }
   ],
   "source": [
    "dataset = LDABERT2Dataset(dataset_type=dataset_type,\n",
    "                           pct_data=0.005,\n",
    "                          max_seq_length=128,\n",
    "                           max_segment_length=300,\n",
    "                           augment_pct=0)\n",
    "\n",
    "sentences, tokenized_sentences, labels = dataset.process()\n",
    "\n",
    "# vectors_path = '../data/clinical_vectors/lda_bert_{}_{}.pkl'.format(dataset_type, len(sentences))\n",
    "vectors_path = '../../data/lda_bert_2/{}/{}_{}.pkl'.format(dataset_type, \n",
    "                                                        dataset.pct_data, \n",
    "                                                        dataset.augment_pct)\n",
    "\n",
    "saved_vectors, saved_labels, saved_sentences, saved_tokenized_sentences = dataset.get_saved_vectors(vectors_path)\n",
    "\n",
    "if len(saved_vectors) == 0:\n",
    "    saved_vectors, saved_labels, saved_sentences, saved_tokenized_sentences = dataset.create_vectors(vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d85d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2695309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input, mid_input, right_input = dataset.format_sentences_tri_input_plus(saved_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d241f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_left_input, lda_mid_input, lda_right_input = dataset.format_sentences_tri_input(saved_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440e8a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([130, 76]), (130, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences['input_ids'].shape, saved_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5771",
   "metadata": {},
   "source": [
    "## Tri Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a2c1248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "context_encoder = ContextEncoder(final_dropout=0.5, \n",
    "                                 dense_neurons=128, \n",
    "                                 max_sentence_length=128, \n",
    "                                 bert_trainable=False,\n",
    "                                 gamma=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19546420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_for_test_input(arr):\n",
    "    return arr[:5]\n",
    "\n",
    "def trim_encoded_for_test_input(arr):\n",
    "    return {\n",
    "        'input_ids': arr['input_ids'][:5],\n",
    "        'token_type_ids': arr['token_type_ids'][:5],\n",
    "        'attention_mask': arr['attention_mask'][:5]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b998eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller test vectors to init model\n",
    "test_left_input = trim_encoded_for_test_input(left_input)\n",
    "test_mid_input = trim_encoded_for_test_input(mid_input)\n",
    "test_right_input = trim_encoded_for_test_input(right_input)\n",
    "test_lda_left_input = trim_for_test_input(lda_left_input)\n",
    "test_lda_mid_input = trim_for_test_input(lda_mid_input)\n",
    "test_lda_right_input = trim_for_test_input(lda_right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76644b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[0.6757858 ],\n",
       "       [0.5165765 ],\n",
       "       [0.72419494],\n",
       "       [0.7739043 ],\n",
       "       [0.58298475]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_encoder([test_left_input, test_mid_input, test_right_input, test_lda_left_input, test_lda_mid_input, test_lda_right_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52b0492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_encoder_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tf_bert_model_1 (TFBertModel multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dense_input_left (Dense)     multiple                  99712     \n",
      "_________________________________________________________________\n",
      "dense_input_mid (Dense)      multiple                  99712     \n",
      "_________________________________________________________________\n",
      "dense_input_right (Dense)    multiple                  99712     \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         multiple                  385       \n",
      "_________________________________________________________________\n",
      "final_dropout (Dropout)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 109,781,761\n",
      "Trainable params: 299,521\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "context_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4cd429a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (12, 64, 768) and (768, 768) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-12ee619b9648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcontext_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(\"No checkpoint available.\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2203\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2205\u001b[1;33m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2206\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2207\u001b[0m         raise NotImplementedError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1335\u001b[0m         options=options)\n\u001b[0;32m   1336\u001b[0m     base.CheckpointPosition(\n\u001b[1;32m-> 1337\u001b[1;33m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[0;32m   1338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m     \u001b[1;31m# Attached dependencies are not attached to the root, so should be restored\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, trackable)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[1;34m(self, checkpoint_position)\u001b[0m\n\u001b[0;32m    971\u001b[0m     restore_ops.extend(\n\u001b[0;32m    972\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[1;32m--> 973\u001b[1;33m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[0;32m    974\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[1;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[0;32m    306\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[0;32m    307\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[1;32m--> 308\u001b[1;33m           validated_saveables).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[0;32m    309\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m       \u001b[0mrestore_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    319\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m           \u001b[0mrestore_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    114\u001b[0m                                           structured_restored_tensors):\n\u001b[0;32m    115\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[1;32m--> 116\u001b[1;33m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    130\u001b[0m       \u001b[0mrestored_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[1;32m--> 132\u001b[1;33m           self.handle_op, self._var_shape, restored_tensor)\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mshape_safe_assign_variable_handle\u001b[1;34m(handle, shape, value, name)\u001b[0m\n\u001b[0;32m    305\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[0mvalue_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m   \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m   return gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m    309\u001b[0m       handle, value_tensor, name=name)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \"\"\"\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (12, 64, 768) and (768, 768) are incompatible"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = '../models/LDABERT2/simple/clinical-157-0.01-pct-0.01-aug_WT6OE/no-finetune/checkpoint.ckpt'\n",
    "\n",
    "\n",
    "# try:\n",
    "context_encoder.load_weights(checkpoint_filepath)\n",
    "# except:\n",
    "#     print(\"No checkpoint available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c604490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: _CHECKPOINTABLE_OBJECT_GRAPH (string) []\n",
      "tensor: dense_input_1/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_1/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_2/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_2/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_2/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_2/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_2/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_2/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_3/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_3/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_3/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [128]\n",
      "tensor: dense_input_3/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_3/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_input_3/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [778, 128]\n",
      "tensor: dense_output/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [1]\n",
      "tensor: dense_output/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [1]\n",
      "tensor: dense_output/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [1]\n",
      "tensor: dense_output/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [384, 1]\n",
      "tensor: dense_output/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE (float32) [384, 1]\n",
      "tensor: dense_output/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE (float32) [384, 1]\n",
      "tensor: optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE (float32) []\n",
      "tensor: optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE (float32) []\n",
      "tensor: optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE (float32) []\n",
      "tensor: optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE (int64) []\n",
      "tensor: optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE (float32) []\n",
      "tensor: sbert/bert/embeddings/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/embeddings/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/embeddings/embeddings/.ATTRIBUTES/VARIABLE_VALUE (float32) [512, 768]\n",
      "tensor: sbert/bert/embeddings/token_type_embeddings/.ATTRIBUTES/VARIABLE_VALUE (float32) [2, 768]\n",
      "tensor: sbert/bert/embeddings/weight/.ATTRIBUTES/VARIABLE_VALUE (float32) [30522, 768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/0/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/0/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/0/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/0/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/1/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/1/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/1/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/1/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/10/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/10/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/10/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/10/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/11/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/11/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/11/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/11/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/2/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/2/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/2/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/2/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/3/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/3/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/3/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/3/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/4/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/4/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/4/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/4/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/5/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/5/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/5/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/5/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/6/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/6/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/6/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/6/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/7/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/7/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/7/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/7/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/8/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/8/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/8/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/8/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/dense_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/dense_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/dense_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/dense_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/self_attention/key/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/self_attention/key/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/self_attention/query/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/self_attention/query/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/self_attention/value/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/attention/self_attention/value/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "tensor: sbert/bert/encoder/layer/9/bert_output/LayerNorm/beta/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/bert_output/LayerNorm/gamma/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/bert_output/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/encoder/layer/9/bert_output/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072, 768]\n",
      "tensor: sbert/bert/encoder/layer/9/intermediate/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [3072]\n",
      "tensor: sbert/bert/encoder/layer/9/intermediate/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 3072]\n",
      "tensor: sbert/bert/pooler/dense/bias/.ATTRIBUTES/VARIABLE_VALUE (float32) [768]\n",
      "tensor: sbert/bert/pooler/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE (float32) [768, 768]\n",
      "# Total number of params: 110380809\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "print_tensors_in_checkpoint_file(file_name=checkpoint_filepath, tensor_name='', all_tensors=False,\n",
    "                                     all_tensor_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66881c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b718000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b15556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef699155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b5bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6ef0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ba592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bcd5721",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d68092d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = ContextEncoder(final_dropout=0.5, \n",
    "                         dense_neurons=64, \n",
    "                         max_sentence_length=128, \n",
    "                         gamma=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14939ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91574f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "021f90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# balanced = balanced binary crossentropy\n",
    "checkpoint_filepath = '../models/LDABERT2/simple/{}-{}-{}-pct-{}-aug/checkpoint.ckpt'.format(\n",
    "                        dataset.dataset_type,                    \n",
    "                        len(sentences), \n",
    "                        dataset.pct_data,\n",
    "                        dataset.augment_pct)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "#     early_stopping,\n",
    "    model_checkpoint_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0d0170c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/LDABERT2/simple/clinical-1622-0.1-pct-0.1-aug/checkpoint.ckpt'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d5e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fb95835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"No checkpoint available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df816c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1642, 1), dtype=int32, numpy=\n",
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.convert_to_tensor(saved_labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7aedf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      " 14/370 [>.............................] - ETA: 27:35 - loss: 0.7415 - accuracy: 0.4788"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-575f2df1cac5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#                     class_weight=class_weight,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit([\n",
    "                        left_input, mid_input, right_input, \n",
    "                        lda_left_input, lda_mid_input, lda_right_input\n",
    "                    ], \n",
    "                    tf.convert_to_tensor(saved_labels), \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1, \n",
    "#                     class_weight=class_weight,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b480d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ade9c3",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a47d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiments import get_experiments_json, get_experiments, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b684a829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_type</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>final_dropout</th>\n",
       "      <th>dense_neurons</th>\n",
       "      <th>pct_data</th>\n",
       "      <th>augment_pct</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ldabert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bert_type dataset_type  final_dropout  dense_neurons  pct_data  \\\n",
       "0    ldabert     clinical            0.5             64         1   \n",
       "1    ldabert     clinical            0.5             64         1   \n",
       "2    ldabert     clinical            0.5            256         1   \n",
       "3    ldabert     clinical            0.5            256         1   \n",
       "4    ldabert         wiki            0.5             64         1   \n",
       "5    ldabert         wiki            0.5             64         1   \n",
       "6    ldabert         wiki            0.5            256         1   \n",
       "7    ldabert         wiki            0.5            256         1   \n",
       "8    ldabert      fiction            0.5             64         1   \n",
       "9    ldabert      fiction            0.5             64         1   \n",
       "10   ldabert      fiction            0.5            256         1   \n",
       "11   ldabert      fiction            0.5            256         1   \n",
       "\n",
       "    augment_pct  epochs  \n",
       "0           0.5    1000  \n",
       "1           1.0    1000  \n",
       "2           0.5    1000  \n",
       "3           1.0    1000  \n",
       "4           0.5    1000  \n",
       "5           1.0    1000  \n",
       "6           0.5    1000  \n",
       "7           1.0    1000  \n",
       "8           0.5    1000  \n",
       "9           1.0    1000  \n",
       "10          0.5    1000  \n",
       "11          1.0    1000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_config = get_experiments_json('ldabert_simple_test')\n",
    "epxeriments_config_df = pd.DataFrame.from_dict(experiments_config)\n",
    "epxeriments_config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0f1e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_type</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>finetune_bert</th>\n",
       "      <th>pct_data</th>\n",
       "      <th>augment_pct</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert</td>\n",
       "      <td>clinical</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert</td>\n",
       "      <td>wiki</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bert_type dataset_type  finetune_bert  pct_data  augment_pct  epochs\n",
       "0    albert     clinical           True         1          0.1     200\n",
       "1    albert     clinical           True         1          0.5     200\n",
       "2    albert     clinical           True         1          1.0     200\n",
       "3    albert      fiction           True         1          0.1     200\n",
       "4    albert      fiction           True         1          0.5     200\n",
       "5    albert      fiction           True         1          1.0     200\n",
       "6    albert         wiki           True         1          0.1     200\n",
       "7    albert         wiki           True         1          0.5     200\n",
       "8    albert         wiki           True         1          1.0     200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read local `config.toml` file.\n",
    "config = get_experiments('ALBERT_FINETUNE_SIMPLE')\n",
    "config_df = pd.DataFrame.from_dict(config)\n",
    "config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e7f80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df.to_csv(r'../models/experiment.csv', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a65dfad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcebcc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for experiment in config:\n",
    "    bert_type = config['bert_type']\n",
    "    dataset_type = config['dataset_type']\n",
    "    finetune_bert = config['finetune_bert']\n",
    "    pct_data = config['pct_data']\n",
    "    augment_pct = config['augment_pct']\n",
    "    epochs = config['epochs']\n",
    "    print(\"params:\", bert_type, dataset_type, finetune_bert, pct_data, augment_pct, epochs)\n",
    "    \n",
    "    # init model\n",
    "    print(\"initializing model...\")\n",
    "    model = ContextEncoder(final_dropout=0.5,\n",
    "                           dense_neurons=64,\n",
    "                           bert_trainable=finetune_bert,\n",
    "                           bert_type=\"albert-base-v2\")\n",
    "    \n",
    "    # init dataset\n",
    "    print(\"initializing dataset...\")\n",
    "    dataset = AlbertDataset(dataset_type=dataset_type,\n",
    "                           pct_data=pct_data,\n",
    "                           max_segment_length=5,\n",
    "                           augment_pct=augment_pct)\n",
    "    \n",
    "    # process dataset\n",
    "    print(\"processing dataset...\")\n",
    "    sentences, tokenized_sentences, labels = dataset.process()\n",
    "    \n",
    "    # create checkpoint path\n",
    "    checkpoint_filepath = '../models/ALBERT/finetune/simple/{}-{}-{}-pct-{}-aug/checkpoint'.format(\n",
    "                            dataset_type,                    \n",
    "                            len(sentences), \n",
    "                            pct_data,\n",
    "                            augment_pct)\n",
    "    print(checkpoint_filepath)\n",
    "    \n",
    "    # compiling model\n",
    "    print(\"compiling the model...\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    try:\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        print(\"model loaded.\")\n",
    "    except:\n",
    "        print(\"No checkpoint available.\")\n",
    "    \n",
    "    # \n",
    "    print(\"starting the training process...\")\n",
    "    history = model.fit(dataset.format_sentences_tri_input(tokenized_sentences), \n",
    "                        tf.convert_to_tensor(labels), \n",
    "                        epochs=EPOCHS,\n",
    "                        validation_split=0.1,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=1, \n",
    "                        # class_weight=class_weight,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    # assigning history to experiment object for saving.\n",
    "    experiment[\"history\"] = history\n",
    "    \n",
    "    print(\"saving results...\")\n",
    "    save_results(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dd2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd] *",
   "language": "python",
   "name": "conda-env-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
