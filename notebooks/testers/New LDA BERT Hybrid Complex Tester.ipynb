{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b816077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e50641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb492ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.insert(0, config.root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811f4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python import keras\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.encoders.context_encoder_ldabert_2 import ContextEncoderSimple, ContextEncoderComplex\n",
    "from src.dataset.ldabert_2 import LDABERT2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975786e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7808d62a",
   "metadata": {},
   "source": [
    "## LDA BERT 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3a05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LDABERT2Dataset(dataset_type=\"clinical\",\n",
    "                       pct_data=0.001,\n",
    "                          max_seq_length=128,\n",
    "                       max_segment_length=5,\n",
    "                       augment_pct=0.001,\n",
    "                         split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5557f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, tokenized_sentences, labels = dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e8c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "sentences length 10\n",
      "10.0 %\r",
      "20.0 %\r",
      "30.0 %\r",
      "40.0 %\r",
      "50.0 %\r",
      "60.0 %\r",
      "70.0 %\r",
      "80.0 %\r",
      "90.0 %\r",
      "100.0 %\r",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 10\n"
     ]
    }
   ],
   "source": [
    "lda_sentences, lda_token_lists, lda_new_labels = dataset.preprocess_lda(sentences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5771",
   "metadata": {},
   "source": [
    "## Tri Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2c1248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "context_encoder = ContextEncoder(final_dropout=0.5, dense_neurons=64, max_sentence_length=128, gamma=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c750e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = [['This is an example sentence', 'This is a new sentence', 'And this is a continuation'], \n",
    "#              ['This is a new sentence', 'And this is a continuation', 'And another continuation'], \n",
    "#              ['And this is a continuation', 'And another continuation', 'And yet another continuation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76644b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d893c81db81a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcontext_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleft_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_left_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_mid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_right_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\\src\\encoders\\context_encoder_ldabert_2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mbert_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mlda_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Compute token embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "context_encoder([[left_input, mid_input, right_input, lda_left_input, lda_mid_input, lda_right_input]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c52b0492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_encoder_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tf_bert_model_24 (TFBertMode multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dense_input_left (Dense)     multiple                  49856     \n",
      "_________________________________________________________________\n",
      "dense_input_mid (Dense)      multiple                  49856     \n",
      "_________________________________________________________________\n",
      "dense_input_right (Dense)    multiple                  49856     \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         multiple                  193       \n",
      "_________________________________________________________________\n",
      "final_dropout (Dropout)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 109,632,001\n",
      "Trainable params: 109,632,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "context_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c604490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66881c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b718000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b15556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef699155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b5bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6ef0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ba592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13a07e6",
   "metadata": {},
   "source": [
    "## Generate Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5575373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\train\\\\clinical\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 15051\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 15051\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 15051 15051 15051\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\test\\\\clinical\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 1071\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 1071\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 1071 1071 1071\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\train\\\\wiki\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 7732\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 7732\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 7732 7732 7732\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\test\\\\wiki\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 2584\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 2584\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 2584 2584 2584\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\train\\\\fiction\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 15818\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 15818\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 15818 15818 15818\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\test\\\\fiction\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 2838\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 2838\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 2838 2838 2838\n"
     ]
    }
   ],
   "source": [
    "dataset_types = [\"clinical\", \"wiki\", \"fiction\"]\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "\n",
    "for d in dataset_types:\n",
    "    for split in dataset_splits:\n",
    "        dataset = LDABERT2Dataset(dataset_type=d,\n",
    "                               pct_data=1,\n",
    "                               max_segment_length=5,\n",
    "                               augment_pct=1,\n",
    "                             split=split)\n",
    " \n",
    "        sentences, tokenized_sentences, labels = dataset.process()\n",
    "\n",
    "        # vectors_path = '../data/clinical_vectors/lda_bert_{}_{}.pkl'.format(dataset_type, len(sentences))\n",
    "        vectors_filename = '{}_{}.pkl'.format(dataset.pct_data, dataset.augment_pct)\n",
    "\n",
    "        saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.get_saved_vectors(split, dataset.dataset_type, vectors_filename)\n",
    "\n",
    "        if len(saved_vectors) == 0:\n",
    "            saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.create_vectors(split, dataset.dataset_type, vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f82d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LDABERTDataset(dataset_type=\"clinical\",\n",
    "                       pct_data=1,\n",
    "                       max_segment_length=5,\n",
    "                       augment_pct=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e7aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, tokenized_sentences, labels = dataset.process(preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f25364d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb35b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a203d1",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e627399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b31d578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\clinical\\\\0.001_0.001.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 10\n",
      "10.0 %\r",
      "20.0 %\r",
      "30.0 %\r",
      "40.0 %\r",
      "50.0 %\r",
      "60.0 %\r",
      "70.0 %\r",
      "80.0 %\r",
      "90.0 %\r",
      "100.0 %\r",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 10\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 10 10 10\n"
     ]
    }
   ],
   "source": [
    "# vectors_path = '../data/clinical_vectors/lda_bert_{}_{}.pkl'.format(dataset_type, len(sentences))\n",
    "vectors_filename = '{}_{}.pkl'.format(dataset.pct_data, dataset.augment_pct)\n",
    "\n",
    "saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.get_saved_vectors(split, dataset.dataset_type, vectors_filename)\n",
    "\n",
    "if len(saved_vectors) == 0:\n",
    "    saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.create_vectors(split, dataset.dataset_type, vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "616fbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input, mid_input, right_input = dataset.format_sentences_tri_input_plus(saved_tokenized_sentences)\n",
    "lda_left_input, lda_mid_input, lda_right_input = dataset.format_sentences_tri_input(saved_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b855c408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(10, 55), dtype=int32, numpy=\n",
       " array([[  101,  2065,  2017,  2342,  2393,  2007,  2115,  5544,  1010,\n",
       "          3531,  2123,  1005,  1056,  2022, 11004,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1037,  1059, 21030,  4371,  2003,  1037,  2152,  8219,\n",
       "          3315, 13896, 25090,  3560, 11192,  2614,  2550,  2011,  2250,\n",
       "         12314,  2083,  2019, 19470,  2135,  8061,  2030, 16620, 13095,\n",
       "          2008,  2788,  5158,  2083,  2019,  2330,  6357,  2030,  2019,\n",
       "          4722, 27208,  1999,  1996, 11192, 17790,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  1059, 21030,  4371,  2003,  1037,  2152,  8219,\n",
       "         10964,  6740,  2614,  7013,  2011,  1037, 13212,  2075,  2030,\n",
       "         13212,  2075, 13212,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2065,  1996,  3612,  2003,  2025, 11221,  1996,  1059,\n",
       "         21030,  4371,  2097,  2693,  1998,  2009,  2003,  2411,  5642,\n",
       "          2011,  1037,  7263, 23893,  1010,  1037,  1055, 17048,  3723,\n",
       "         19340,  1010,  1998,  2019, 16903, 23893,  2075,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2009,  2515,  2025,  5373,  5258,  2005,  1059, 21030,\n",
       "         11254,  2000,  4503,  2076,  2037,  2607,  1998,  2024,  2411,\n",
       "         11677,  2011,  3576, 21454,  2012,  1037, 12430,  3446,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2174,  1010,  1059, 21030,  4371,  2458,  2064,  5258,\n",
       "          2130,  2096,  5505,  1999, 14163,  7874,  2008,  2038,  2042,\n",
       "          2081,  2039,  1997,  8331,  1998, 14163,  7874, 24972,  1010,\n",
       "          3391,  2012,  1996,  4515,  1997,  1996, 23245,  2015,  1998,\n",
       "          2012,  1996,  3903,  1997,  1996,  8948,  1010,  2107,  2004,\n",
       "          2043, 14163,  7874, 24972,  2024,  6086,  2000,  2250,  1012,\n",
       "           102],\n",
       "        [  101,  1040,  7274, 19362, 13765,  6200,  2003,  9145,  2030,\n",
       "          8796,  4424, 23198,  1998,  2064,  2191,  1996,  2132,  2514,\n",
       "          2066,  1037, 15116,  1999,  2115,  2132,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2009,  2003,  2524,  2000,  3275,  2041,  2054,  2003,\n",
       "          4786,  2009,  1010,  2021,  2009,  2003,  5791,  2825,  2000,\n",
       "          2131,  2009,  1999,  1996,  2034,  2173,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1045,  2572,  1037,  2502,  5470,  1997,  1996,  8789,\n",
       "          7804,  1998,  2052, 16755,  2014,  2000,  3087,  2007,  5544,\n",
       "          2055,  2129,  2027,  3066,  2007,  2023,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2016,  2064,  2393,  2017,  1999,  2115,  5544,  1998,\n",
       "          2016,  2064,  2393,  2131,  2115,  3086,  2041,  1997,  1037,\n",
       "         10103,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]])>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(10, 55), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n",
       " 'attention_mask': <tf.Tensor: shape=(10, 55), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd5721",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d68092d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = ContextEncoderComplex(final_dropout=0.8,\n",
    "                               dense_neurons=256,\n",
    "                             lstm_size=256,\n",
    "                             lstm_dropout_percentage=0.75,\n",
    "                             cnn_filters=8,\n",
    "                             cnn_kernel_size=3,\n",
    "                             pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14939ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91574f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "021f90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# balanced = balanced binary crossentropy\n",
    "checkpoint_filepath = '../models/LDABERT/complex/{}-{}-{}-pct-{}-aug/checkpoint.ckpt'.format(\n",
    "                        dataset.dataset_type,                    \n",
    "                        len(sentences), \n",
    "                        dataset.pct_data,\n",
    "                        dataset.augment_pct)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "#     early_stopping,\n",
    "    model_checkpoint_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d0170c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/LDABERT/complex/clinical-10-0.001-pct-0.001-aug/checkpoint.ckpt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d5e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fb95835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"No checkpoint available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7aedf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.5697 - accuracy: 0.7143 - val_loss: 0.5432 - val_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 0.5244 - accuracy: 0.7143 - val_loss: 0.5253 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 0.5260 - accuracy: 0.7143 - val_loss: 0.5038 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 0.5324 - accuracy: 0.7143 - val_loss: 0.4806 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "history = model.fit([\n",
    "                        left_input, mid_input, right_input, \n",
    "                        lda_left_input, lda_mid_input, lda_right_input\n",
    "                    ], \n",
    "                    tf.convert_to_tensor(saved_labels), \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.25,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1, \n",
    "#                         class_weight=class_weight,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b480d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dd2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
