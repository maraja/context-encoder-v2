{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b816077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e50641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb492ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.insert(0, config.root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python import keras\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.encoders.context_encoder_ldabert_2 import ContextEncoderSimple, ContextEncoderComplex\n",
    "from src.dataset.ldabert_2 import LDABERT2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975786e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7808d62a",
   "metadata": {},
   "source": [
    "## LDA BERT 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LDABERT2Dataset(dataset_type=\"clinical\",\n",
    "                       pct_data=0.001,\n",
    "                          max_seq_length=128,\n",
    "                       max_segment_length=5,\n",
    "                       augment_pct=0.001,\n",
    "                         split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5557f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial segments False\n",
      "artificial segments False\n"
     ]
    }
   ],
   "source": [
    "sentences, tokenized_sentences, labels = dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e8c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "sentences length 10\n",
      "10.0 %\r",
      "20.0 %\r",
      "30.0 %\r",
      "40.0 %\r",
      "50.0 %\r",
      "60.0 %\r",
      "70.0 %\r",
      "80.0 %\r",
      "90.0 %\r",
      "100.0 %\r",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 10\n"
     ]
    }
   ],
   "source": [
    "lda_sentences, lda_token_lists, lda_new_labels = dataset.preprocess_lda(sentences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5771",
   "metadata": {},
   "source": [
    "## Tri Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2c1248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "context_encoder = ContextEncoder(final_dropout=0.5, dense_neurons=64, max_sentence_length=128, gamma=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c750e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = [['This is an example sentence', 'This is a new sentence', 'And this is a continuation'], \n",
    "#              ['This is a new sentence', 'And this is a continuation', 'And another continuation'], \n",
    "#              ['And this is a continuation', 'And another continuation', 'And yet another continuation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76644b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d893c81db81a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcontext_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleft_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_left_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_mid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_right_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\phd\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\\src\\encoders\\context_encoder_ldabert_2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mbert_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mlda_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Compute token embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "context_encoder([[left_input, mid_input, right_input, lda_left_input, lda_mid_input, lda_right_input]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c52b0492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"context_encoder_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tf_bert_model_24 (TFBertMode multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dense_input_left (Dense)     multiple                  49856     \n",
      "_________________________________________________________________\n",
      "dense_input_mid (Dense)      multiple                  49856     \n",
      "_________________________________________________________________\n",
      "dense_input_right (Dense)    multiple                  49856     \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         multiple                  193       \n",
      "_________________________________________________________________\n",
      "final_dropout (Dropout)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 109,632,001\n",
      "Trainable params: 109,632,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "context_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c604490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66881c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b718000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b15556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef699155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b5bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6ef0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ba592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13a07e6",
   "metadata": {},
   "source": [
    "## Generate Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5575373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\train\\\\clinical\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 15051\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 15051\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 15051 15051 15051\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\test\\\\clinical\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 1071\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 1071\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 1071 1071 1071\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\train\\\\wiki\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 7732\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 7732\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 7732 7732 7732\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\test\\\\wiki\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 2584\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 2584\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 2584 2584 2584\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\train\\\\fiction\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 15818\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 15818\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 15818 15818 15818\n",
      "something went wrong [Errno 2] No such file or directory: 'C:\\\\Users\\\\Computer\\\\Google Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2\\\\data\\\\lda_bert_2\\\\generated_vectors\\\\test\\\\fiction\\\\1_1.pkl'\n",
      "root path C:\\Users\\Computer\\Google Drive\\SCHOOL\\PhD\\Code\\context-encoder-v2\n",
      "Preprocessing raw texts ...\n",
      "sentences length 2838\n",
      "Preprocessing raw texts. Done!\n",
      "lda sentences length 2838\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "saving vectors... 2838 2838 2838\n"
     ]
    }
   ],
   "source": [
    "dataset_types = [\"clinical\", \"wiki\", \"fiction\"]\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "\n",
    "for d in dataset_types:\n",
    "    for split in dataset_splits:\n",
    "        dataset = LDABERT2Dataset(dataset_type=d,\n",
    "                               pct_data=1,\n",
    "                               max_segment_length=5,\n",
    "                               augment_pct=1,\n",
    "                             split=split)\n",
    " \n",
    "        sentences, tokenized_sentences, labels = dataset.process()\n",
    "\n",
    "        # vectors_path = '../data/clinical_vectors/lda_bert_{}_{}.pkl'.format(dataset_type, len(sentences))\n",
    "        vectors_filename = '{}_{}.pkl'.format(dataset.pct_data, dataset.augment_pct)\n",
    "\n",
    "        saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.get_saved_vectors(split, dataset.dataset_type, vectors_filename)\n",
    "\n",
    "        if len(saved_vectors) == 0:\n",
    "            saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.create_vectors(split, dataset.dataset_type, vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f82d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LDABERTDataset(dataset_type=\"clinical\",\n",
    "                       pct_data=1,\n",
    "                       max_segment_length=5,\n",
    "                       augment_pct=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e7aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, tokenized_sentences, labels = dataset.process(preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f25364d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb35b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a203d1",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e627399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\SCHOOL\\\\PhD\\\\Code\\\\context-encoder-v2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31d578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors_path = '../data/clinical_vectors/lda_bert_{}_{}.pkl'.format(dataset_type, len(sentences))\n",
    "vectors_filename = '{}_{}.pkl'.format(dataset.pct_data, dataset.augment_pct)\n",
    "\n",
    "saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.get_saved_vectors(\"train\", dataset.dataset_type, vectors_filename)\n",
    "\n",
    "if len(saved_vectors) == 0:\n",
    "    saved_vectors, saved_tokens, saved_labels, saved_tokenized_sentences = dataset.create_vectors(\"train\", dataset.dataset_type, vectors_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616fbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input, mid_input, right_input = dataset.format_sentences_tri_input_plus(saved_tokenized_sentences)\n",
    "lda_left_input, lda_mid_input, lda_right_input = dataset.format_sentences_tri_input(saved_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b855c408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(10, 55), dtype=int32, numpy=\n",
       " array([[  101,  2065,  2017,  2342,  2393,  2007,  2115,  5544,  1010,\n",
       "          3531,  2123,  1005,  1056,  2022, 11004,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1037,  1059, 21030,  4371,  2003,  1037,  2152,  8219,\n",
       "          3315, 13896, 25090,  3560, 11192,  2614,  2550,  2011,  2250,\n",
       "         12314,  2083,  2019, 19470,  2135,  8061,  2030, 16620, 13095,\n",
       "          2008,  2788,  5158,  2083,  2019,  2330,  6357,  2030,  2019,\n",
       "          4722, 27208,  1999,  1996, 11192, 17790,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  1059, 21030,  4371,  2003,  1037,  2152,  8219,\n",
       "         10964,  6740,  2614,  7013,  2011,  1037, 13212,  2075,  2030,\n",
       "         13212,  2075, 13212,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2065,  1996,  3612,  2003,  2025, 11221,  1996,  1059,\n",
       "         21030,  4371,  2097,  2693,  1998,  2009,  2003,  2411,  5642,\n",
       "          2011,  1037,  7263, 23893,  1010,  1037,  1055, 17048,  3723,\n",
       "         19340,  1010,  1998,  2019, 16903, 23893,  2075,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2009,  2515,  2025,  5373,  5258,  2005,  1059, 21030,\n",
       "         11254,  2000,  4503,  2076,  2037,  2607,  1998,  2024,  2411,\n",
       "         11677,  2011,  3576, 21454,  2012,  1037, 12430,  3446,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2174,  1010,  1059, 21030,  4371,  2458,  2064,  5258,\n",
       "          2130,  2096,  5505,  1999, 14163,  7874,  2008,  2038,  2042,\n",
       "          2081,  2039,  1997,  8331,  1998, 14163,  7874, 24972,  1010,\n",
       "          3391,  2012,  1996,  4515,  1997,  1996, 23245,  2015,  1998,\n",
       "          2012,  1996,  3903,  1997,  1996,  8948,  1010,  2107,  2004,\n",
       "          2043, 14163,  7874, 24972,  2024,  6086,  2000,  2250,  1012,\n",
       "           102],\n",
       "        [  101,  1040,  7274, 19362, 13765,  6200,  2003,  9145,  2030,\n",
       "          8796,  4424, 23198,  1998,  2064,  2191,  1996,  2132,  2514,\n",
       "          2066,  1037, 15116,  1999,  2115,  2132,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2009,  2003,  2524,  2000,  3275,  2041,  2054,  2003,\n",
       "          4786,  2009,  1010,  2021,  2009,  2003,  5791,  2825,  2000,\n",
       "          2131,  2009,  1999,  1996,  2034,  2173,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1045,  2572,  1037,  2502,  5470,  1997,  1996,  8789,\n",
       "          7804,  1998,  2052, 16755,  2014,  2000,  3087,  2007,  5544,\n",
       "          2055,  2129,  2027,  3066,  2007,  2023,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  2016,  2064,  2393,  2017,  1999,  2115,  5544,  1998,\n",
       "          2016,  2064,  2393,  2131,  2115,  3086,  2041,  1997,  1037,\n",
       "         10103,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]])>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(10, 55), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n",
       " 'attention_mask': <tf.Tensor: shape=(10, 55), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd5721",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68092d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = ContextEncoderComplex(final_dropout=0.8,\n",
    "                               dense_neurons=256,\n",
    "                             lstm_size=256,\n",
    "                             lstm_dropout_percentage=0.75,\n",
    "                             cnn_filters=8,\n",
    "                             cnn_kernel_size=3,\n",
    "                             pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91574f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021f90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# balanced = balanced binary crossentropy\n",
    "checkpoint_filepath = '../models/LDABERT/complex/{}-{}-{}-pct-{}-aug/checkpoint.ckpt'.format(\n",
    "                        dataset.dataset_type,                    \n",
    "                        len(sentences), \n",
    "                        dataset.pct_data,\n",
    "                        dataset.augment_pct)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "#     early_stopping,\n",
    "    model_checkpoint_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d0170c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/LDABERT/complex/clinical-10-0.001-pct-0.001-aug/checkpoint.ckpt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d5e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb95835",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "except:\n",
    "    print(\"No checkpoint available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7aedf8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node binary_crossentropy/Cast (defined at <ipython-input-22-58df2e6e0a4a>:3) ]] [Op:__inference_train_function_49855]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-58df2e6e0a4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# remove warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = model.fit([\n\u001b[0m\u001b[0;32m      4\u001b[0m                         \u001b[0mleft_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                         \u001b[0mlda_left_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_mid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_right_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node binary_crossentropy/Cast (defined at <ipython-input-22-58df2e6e0a4a>:3) ]] [Op:__inference_train_function_49855]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "history = model.fit([\n",
    "                        left_input, mid_input, right_input, \n",
    "                        lda_left_input, lda_mid_input, lda_right_input\n",
    "                    ], \n",
    "                    tf.convert_to_tensor(saved_labels), \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.25,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1, \n",
    "#                         class_weight=class_weight,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b480d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dd2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
